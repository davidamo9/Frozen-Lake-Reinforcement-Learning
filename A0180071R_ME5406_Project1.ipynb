{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e2cedd0",
   "metadata": {},
   "source": [
    "Instruction:\n",
    "\n",
    "Code Components:\n",
    "1. Initialisation + helper functions\n",
    "2. Setting up Environment for task1 (env)\n",
    "3. Setting up Environment for Extended Implementation (env_large)\n",
    "4. Function for First Visit Monte Carlo Control without exploring starts\n",
    "5. Function for SARSA with e-Greedy Behaviour Policy\n",
    "6. Function for Q-learning with e-Greedy Behavior Policy\n",
    "7. Testing function\n",
    "Onwards: Training commands for 2 environents using 3 methods = 6 commands in total.\n",
    "\n",
    "Install pip gym, numpy and pygame if you haven't. Run cells 1 to 7 and then you are ready to train and test. Pick the correct command out of the 6 commands at the end to train. Training parameters need to be adjusted within the functions in 4,5,6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "587cd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gym, numpy and pygame\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#helper functions\n",
    "#Setting the rewards for reaching the hole states as -1 by updating the transition function\n",
    "def update_transition_fn(env,hole_states):\n",
    "    num_states = env.observation_space.n                         #number of states\n",
    "    num_actions = env.action_space.n                             #number of actions\n",
    "    for s in range(num_states):\n",
    "        for a in range(num_actions):\n",
    "            transitions = env.env.P[s][a]                        #transition function: contains data of probability of transition,next state,reward,bool indicating termination state or not\n",
    "            for p_trans, next_s, r, done in transitions:\n",
    "                if done and next_s in hole_states:               #if it is termination state and hole states\n",
    "                    # Set the reward for falling into a hole to -1\n",
    "                    r = -1\n",
    "                elif done and next_s == num_states:              #if it is termination state and goal \n",
    "                    r = 1\n",
    "                else:\n",
    "                    r = 0\n",
    "                env.env.P[s][a] = [(p_trans, next_s, r, done)]   #update the transition function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef322ac",
   "metadata": {},
   "source": [
    "Task 1: Setting up the Frozen Lake Environments (Basic Implementation). Run the following cell to set up the 4x4 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11586dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the environment using gym from openAI \n",
    "env=gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False,render_mode=\"human\")\n",
    "\n",
    "# Modify the reward table: reward when falling into the hole to -1 instead of 0.\n",
    "env.env.reward_range = (-1,1)\n",
    "env.env.rewards = {\n",
    "    b'G' : 1,\n",
    "    b'H' : -1,\n",
    "    b'F' : 0,\n",
    "    b'S' : 0\n",
    "}\n",
    "#get the states of the holes\n",
    "hole_states = []                                      #Empty array to store the indices of holes\n",
    "for i in range(len(env.desc)):                        #looping through each row\n",
    "    for j in range(len(env.desc[0])):                 #looping through each column \n",
    "        if env.desc[i][j] == b'H':                    #checking each state if it's a hole\n",
    "            hole_states.append(i*len(env.desc) + j)   #append to the array, state index = (num of rows x len of row) + num of column\n",
    "            \n",
    "#update the transition function\n",
    "update_transition_fn(env,hole_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d847a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'prob': 1})\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(env.reset())\n",
    "print(state[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7b34d",
   "metadata": {},
   "source": [
    "Task 2: Extended Implementation-Setting up FrozenLake ENV with 10x10 grid. Run the following cell to set up the 10x10 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0357a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Generate 10x10 map size with 25% hole to grid ratio, random placement of holes \n",
    "np.random.seed(4) #for reproducibility,change this value to create maps with different layouts of holes\n",
    "size = 10\n",
    "num_holes = 25  #25% of 100 tiles = 25 holes\n",
    "\n",
    "#Creating desc to generate FrozenLake map\n",
    "desc = np.zeros((size, size), dtype='U1')\n",
    "desc[:, :] = b'F'\n",
    "# Set the start and goal positions\n",
    "desc[0, 0] = b'S'\n",
    "desc[-1, -1] = b'G'\n",
    "# Place 25 holes randomly on the map\n",
    "hole_positions = np.random.choice(size*size, num_holes, replace=False)\n",
    "hole_rows, hole_cols = np.unravel_index(hole_positions, (size, size))\n",
    "desc[hole_rows, hole_cols] = b'H'\n",
    "\n",
    "env_large = gym.make('FrozenLake-v1', desc=desc,is_slippery=False,render_mode=\"human\")\n",
    "env_large.env.reward_range = (-1,1)\n",
    "env_large.env.rewards = {\n",
    "    b'G' : 1,\n",
    "    b'H' : -1,\n",
    "    b'F' : 0,\n",
    "    b'S' : 0\n",
    "}\n",
    "update_transition_fn(env_large,hole_positions)\n",
    "#env_large.reset()\n",
    "#env_large.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951532c",
   "metadata": {},
   "source": [
    "Implementing Training methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a3ed8",
   "metadata": {},
   "source": [
    "METHOD 1: FIRST VISIT MONTE CARLO CONTROL WITHOUT EXPLORING STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052134af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST VISIT MONTE CARLO CONTROL WITHOUT EXPLORING STARTS\n",
    "# Function inputs :\n",
    "#     env : environment to be trained on (4x4 or 10x10)\n",
    "#     num_episodes : number of episodes to be trained\n",
    "#     check_convergence : True to check the number of episodes needed to reach convergence\n",
    "\n",
    "# Function outputs:\n",
    "#     policy : the optimal policy after the training process\n",
    "#     converge_epi : the number of episodes until convergence happens, set to 0 if check_converge = False\n",
    "\n",
    "def training_Monte_Carlo(env,num_episodes,check_convergence = True):\n",
    "    # Initialize the Q-table and the returns table with zeros\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    Returns = {(s,a): [] for s in range(env.observation_space.n) for a in range(env.action_space.n)}\n",
    "    \n",
    "    #Training Parameters\n",
    "    gamma = 0.7 # discount factor\n",
    "    epsilon = 0.45 # exploration rate\n",
    "    \n",
    "    #Convergence Criterion\n",
    "    tolerence = 0.01         \n",
    "    start_check = 200         #Start checking after \"input\" episodes\n",
    "    Q_changes = []\n",
    "    converge_epi = 0\n",
    "   \n",
    "    # First-visit Monte Carlo control algorithm\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        current_state = state[0]\n",
    "        done = False\n",
    "        episode = []\n",
    "        Q_prev = Q.copy()                #Storing Q table before it gets updated to check for convergence\n",
    "        \n",
    "        #training parameter control\n",
    "        if i%10 == 0:\n",
    "            epsilon -= 0.001             #gradual decrease of epsilon greedy parameter, adjust accordingly\n",
    "        \n",
    "        # Run the episode and record the visited states, actions, and rewards\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:                          #if the random value is less than epsilon\n",
    "                action = env.action_space.sample()                  #a random action is chosen out of available actions\n",
    "            else:                                                   #otherwise\n",
    "                action = np.argmax(Q[current_state, :])             #an action with the highest Q value is chosen, correct action\n",
    "            next_state, reward, done, _ ,prob = env.step(action)    #take the action and record the new state, reward and other info provided by step()\n",
    "            episode.append((current_state, action, reward))         #record in episode array for calculation later\n",
    "            current_state = next_state                              #update the new state as current state\n",
    "            \n",
    "        # When a termination state is reached\n",
    "        # Update the Q-table using the returns\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):                                       #looping the episode array in reverse order; looking back\n",
    "            current_state, action, reward = episode[t]                                #retrieving data of the episode\n",
    "            G = gamma * G + reward                                                    #Calculate G\n",
    "            if (current_state, action) not in [(x[0],x[1]) for x in episode[0:t]]:    #if state,action pair of the episode is not visited\n",
    "                Returns[(current_state, action)].append(G)                            #Append G to Return\n",
    "                Q[current_state, action] = np.mean(Returns[(current_state, action)])  #Update Q-table with average of Return(state,action)\n",
    "        \n",
    "       \n",
    "        print(f\"  Episode Number: {i}\")      #keeping track of training process\n",
    "        #print(Q)\n",
    "        #Checking for Convergence\n",
    "        if check_convergence != False and i >= start_check:       #Conditions for checking\n",
    "            Q_change = np.max(np.abs(Q_prev - Q))                 #Finding the change in Q table before and after an episode\n",
    "            Q_changes.append(Q_change)                            #Recording the change\n",
    "            if np.mean(Q_changes[-start_check:]) <= tolerence:    #if the mean of the changes is lower than tolerence\n",
    "                converge_epi = i + 1                              #the episode that convergence happens\n",
    "                break                                             \n",
    "    #Generate Policy from the final Q table          \n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    return policy,converge_epi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4084f11",
   "metadata": {},
   "source": [
    "METHOD 2: SARSA WITH e-GREEDY BEHAVIOR POLICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa17f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA WITH AN E-GREEDY BEHAVIOR POLICY\n",
    "# Function inputs :\n",
    "#     env : environment to be trained on (4x4 or 10x10)\n",
    "#     num_episodes : number of episodes to be trained\n",
    "#     check_convergence : True to check the number of episodes needed to reach convergence\n",
    "\n",
    "# Function outputs:\n",
    "#     policy : the optimal policy after the training process\n",
    "#     converge_epi : the number of episodes until convergence happens, set to 0 if check_converge = False\n",
    "\n",
    "\n",
    "def training_SARSA(env,num_episodes,check_convergence = True):\n",
    "    # Initialize the Q-table and the returns table with zeros\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    \n",
    "    #Training Parameters\n",
    "    epsilon = 0.7\n",
    "    alpha = 0.3\n",
    "    gamma = 0.5\n",
    "    \n",
    "    #Convergence Criterion\n",
    "    tolerence = 0.01  \n",
    "    start_check = 20\n",
    "    Q_changes = []\n",
    "    converge_epi = 0\n",
    "\n",
    "    # SARSA algorithm\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        current_state = state[0]     # Convert state tuple to integer index\n",
    "        done = False \n",
    "        Q_prev = Q.copy()        #Storing Q table before it gets updated to check for convergence\n",
    "        print(f\"Episode Number {i}\")\n",
    "        \n",
    "        #Parameter Control\n",
    "        if i%10 == 0:\n",
    "            epsilon -= 0.01         #Gradual decrease of epsilon value\n",
    "            alpha -= 0.01           #Gradual decrease of learning rate: to ensure convergence\n",
    "        \n",
    "        # Choose the initial action using epsilon-greedy policy\n",
    "        if np.random.rand() < epsilon:                   #if a random number [0,1]is lower than epsilon\n",
    "            action = env.action_space.sample()           #a random action is chosen out of available actions\n",
    "        else:                                            #else\n",
    "            action = np.argmax(Q[current_state, :])      #an action with the highest Q value is chosen, correct action\n",
    "        while not done:\n",
    "            # Take the action and observe the next state and reward\n",
    "            next_state, reward, done, _, prob = env.step(action)\n",
    "            # Choose the next action using epsilon-greedy policy\n",
    "            if np.random.rand() < epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action = np.argmax(Q[next_state, :])\n",
    "            # Update the Q-table\n",
    "            current_value = Q[current_state, action]                                        #current Q value\n",
    "            next_max = np.max(Q[next_state, :])                                             #Get the highest Q value of taking the next step\n",
    "            new_value = current_value + alpha * (reward + gamma * next_max - current_value) #calculate a new Q value using the SARSA update rule\n",
    "            Q[current_state, action] = new_value                                            #Update the Q table with the new Q value\n",
    "            current_state = next_state                                                      #update the state to next state\n",
    "            action = next_action                                                            #update the action to next action\n",
    "            print(f\"  State: {current_state}, Action: {action}, Reward: {reward}, Q-value: {new_value}\")\n",
    "        if i % 50 == 0:      #To check Q table during training at every 50 episodes\n",
    "            print(Q)\n",
    "            \n",
    "        #Checking for Convergence\n",
    "        if check_convergence != False and i >= start_check:\n",
    "            Q_change = np.max(np.abs(Q_prev - Q))\n",
    "            Q_changes.append(Q_change)\n",
    "            if np.mean(Q_changes[-start_check:]) <= tolerence:\n",
    "                converge_epi = i + 1\n",
    "                break\n",
    "    print(Q)        \n",
    "    policy = np.argmax(Q, axis=1)      #Generating optimal policy from the final Q table\n",
    "    return policy,converge_epi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f6b200",
   "metadata": {},
   "source": [
    "METHOD 3: Q-LEARNING WITH e-GREEDY BEHAVIOR POLICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db21cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-LEARNING WITH AN E-GREEDY BEHAVIOR POLICY\n",
    "# Function inputs :\n",
    "#     env : environment to be trained on (4x4 or 10x10)\n",
    "#     num_episodes : number of episodes to be trained\n",
    "#     check_convergence : True to check the number of episodes needed to reach convergence\n",
    "\n",
    "# Function outputs:\n",
    "#     policy : the optimal policy after the training process\n",
    "#     converge_epi : the number of episodes until convergence happens, set to 0 if check_converge = False\n",
    "\n",
    "def training_Q(env,num_episodes,check_convergence = True):\n",
    "    # Initialize the Q-table with zeros\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    \n",
    "    #Training Parameters\n",
    "    epsilon = 0.45\n",
    "    alpha = 0.9\n",
    "    gamma = 0.8\n",
    "\n",
    "    #Convergence Criterion\n",
    "    tolerence = 0.01  \n",
    "    start_check = 200\n",
    "    Q_changes = []\n",
    "    converge_epi = 0\n",
    "\n",
    "    # Q-learning algorithm with an epsilon-greedy behavior policy\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        current_state = state[0]\n",
    "        done = False\n",
    "        Q_prev = Q.copy()        #Storing current Q value to check convergence after taking action\n",
    "        print(f\"Episode Number {i}\")\n",
    "        \n",
    "        #Parameter Control\n",
    "        if i%10 == 0:\n",
    "            epsilon -= 0.002\n",
    "            alpha -= 0.01\n",
    "            \n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:                    #Choosing an action based on epsilon greedy policy(same as above: SARSA)\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[current_state, :])\n",
    "                \n",
    "            next_state, reward, done, _ ,prob = env.step(action)  #recording data of the new state, reward and other data\n",
    "            #Update Q table using the Q-learning update rule : updated value =  current value + learning rate x (reward + gamma * max Q value - current Q value)\n",
    "            Q[current_state, action] = Q[current_state, action] + alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[current_state, action])\n",
    "            current_state = next_state                            #update current state as next state\n",
    "            \n",
    "        if i % 50 == 0:      #To check Q table during training at every 50 episodes\n",
    "            print(Q)\n",
    "\n",
    "        #Checking for Convergence\n",
    "        if check_convergence != False and i >= start_check:\n",
    "            Q_change = np.max(np.abs(Q_prev - Q))\n",
    "            Q_changes.append(Q_change)\n",
    "            if np.mean(Q_changes[-start_check:]) <= tolerence:\n",
    "                converge_epi = i + 1\n",
    "                break\n",
    "    print(Q)\n",
    "    policy = np.argmax(Q, axis=1)   #Generate Optimal Policy\n",
    "    return policy,converge_epi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5f4b4",
   "metadata": {},
   "source": [
    "Helper function for testing the optimal path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88257646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the models, get Q tables and policies from each training methods, run the test and render the path\n",
    "def testing(policy):\n",
    "    # Test the learned policy\n",
    "    goal_reached = False\n",
    "    steps = 0\n",
    "    state = env.reset()\n",
    "    current_state = state[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Choose the action with the highest Q-value\n",
    "        #action = np.argmax(Q[state, :])    #use this action instead if you're using Q table and not policy\n",
    "        action = policy[current_state]\n",
    "        # Take the action and observe the next state and reward\n",
    "        next_state, reward, done, _,sth = env.step(action)\n",
    "        steps += 1\n",
    "        current_state = next_state\n",
    "    if reward == 1.0: # goal reached\n",
    "        goal_reached = True\n",
    "        print(f\"Goal is reached in {steps} steps\")\n",
    "    print(\"Goal is not reached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5259e44",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b1b3e",
   "metadata": {},
   "source": [
    "on 4x4 environment, env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246dade",
   "metadata": {},
   "source": [
    "To train the 4x4 frozen lake using First Visit Monte Carlo Control without exploring starts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed17143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_policy,converge_epi = training_Monte_Carlo(env,num_episodes,check_convergence = True/False)\n",
    "policy_small_MC,converge_epi = training_Monte_Carlo(env,200,check_convergence = True)\n",
    "print(policy_small_MC)\n",
    "testing(policy_small_MC)          #to check if the policy reaches the goal by rendering the path\n",
    "\n",
    "#policy_small_MC = np.reshape(policy_small_MC,(4,4))   #to see the policy according to grid shape to find the path visually\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d57db",
   "metadata": {},
   "source": [
    "To train the 4x4 frozen lake using SARSA Algorithm with e-greedy behavior policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf3a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_policy,converge_epi = training_SARSA(env,num_episodes,check_convergence = True/False)\n",
    "policy_small_SS,converge_epi = training_SARSA(env,300,check_convergence = True)\n",
    "print(policy_small_SS)\n",
    "testing(policy_small_SS)          #to check if the policy reaches the goal by rendering the path\n",
    "\n",
    "#policy_small_SS = np.reshape(policy_small_SS,(4,4))   #to see the policy according to grid shape to find the path visually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d37ead",
   "metadata": {},
   "source": [
    "To train the 4x4 frozen lake using Q-learning Algorithm with e-greedy behavior policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b15674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_policy,converge_epi = training_Q(env,num_episodes,check_convergence = True/False)\n",
    "policy_small_Q,converge_epi = training_Q(env,300,check_convergence = True)\n",
    "print(policy_small_Q)\n",
    "testing(policy_small_Q)          #to check if the policy reaches the goal by rendering the path\n",
    "\n",
    "#policy_small_Q = np.reshape(policy_small_Q,(4,4))   #to see the policy according to grid shape to find the path visually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b0d5c7",
   "metadata": {},
   "source": [
    "on 10x10 environment , env_large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd19ced",
   "metadata": {},
   "source": [
    "To train the 10x10 frozen lake using First Visit Monte Carlo Control without exploring starts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3062cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_policy,converge_epi = training_Monte_Carlo(env,num_episodes,check_convergence = True/False)\n",
    "policy_large_MC,converge_epi = training_Monte_Carlo(env,5000,check_convergence = False)\n",
    "print(policy_large_MC)\n",
    "testing(policy_large_MC)          #to check if the policy reaches the goal by rendering the path\n",
    "\n",
    "#policy_large_MC = np.reshape(policy_large_MC,(10,10))   #to see the policy according to grid shape to find the path visually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeca767",
   "metadata": {},
   "source": [
    "To train the 10x10 frozen lake using SARSA Algorithm with e-greedy behavior policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a56d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_policy,converge_epi = training_SARSA(env,num_episodes,check_convergence = True/False)\n",
    "policy_large_SS,converge_epi = training_SARSA(env_large,5000,check_convergence = False)\n",
    "print(policy_large_SS)\n",
    "testing(policy_large_SS)          #to check if the policy reaches the goal by rendering the path\n",
    "\n",
    "#policy_large_SS = np.reshape(policy_large_SS,(10,10))   #to see the policy according to grid shape to find the path visually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c32b28",
   "metadata": {},
   "source": [
    "To train the 10x10 frozen lake using Q-learning Algorithm with e-greedy behavior policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a35e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_policy,converge_epi = training_Q(env,num_episodes,check_convergence = True/False)\n",
    "policy_large_Q,converge_epi = training_Q(env_large,5000,check_convergence = False)\n",
    "print(policy_large_Q)\n",
    "testing(policy_large_Q)          #to check if the policy reaches the goal by rendering the path\n",
    "\n",
    "#policy_large_Q = np.reshape(policy_large_Q,(10,10))   #to see the policy according to grid shape to find the path visually"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
